""" –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–ø–¥–µ–π—Ç–∞ —Å—Ç–µ–Ω–¥–∞ –Ω–∞ –≥—É–≥–ª –¥–∏—Å–∫–µ, —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∏–º–µ—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞–Ω–∞–ª—ã –Ω–∞ –ª–∏—Å—Ç–µ - –ö–∞–Ω–∞–ª—ã
"""
import os
import time
import re
import json
import hashlib
import tempfile
from datetime import datetime, timedelta
from urllib.parse import urlparse
import asyncio


import gspread
from oauth2client.service_account import ServiceAccountCredentials
import requests
from dotenv import load_dotenv
import openai
from openai import OpenAI, RateLimitError, AuthenticationError, PermissionDeniedError
from twelvelabs import TwelveLabs, TooManyRequestsError
from twelvelabs.tasks import TasksRetrieveResponse
from twelvelabs.core.api_error import ApiError

import sys, logging
from logging.handlers import RotatingFileHandler

# ================== CONSTANTS ==================
URL_1 = "https://api.tgstat.ru/channels/get"
URL_2 = "https://api.tgstat.ru/channels/posts"
URL_3 = "https://api.tgstat.ru/posts/stat"

with open('prompts/openai_sys_role.txt', 'r', encoding='utf-8') as f2:
    OPENAI_SYS_ROLE = f2.read().strip()
with open('prompts/pegasus_sys_role.txt', 'r', encoding='utf-8') as f3:
    PEGASUS_SYS_ROLE = f3.read().strip()
with open('prompts/headers.json', 'r', encoding='utf-8') as f4:
    final_headers = json.load(f4)

ADMIN_SPREADSHEET_NAME: str = "Sellebra TGstat (admin)"
CHANNELS: str = '–ö–∞–Ω–∞–ª—ã'
SUGGESTIONS: str ='–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏'
PROFILE: str = '–ü—Ä–æ—Ñ–∏–ª—å'
MAIN: str = 'Main'
LOG: str = 'Log'

# ================== LOGGER ==================
LOGGER_NAME = "analyse_admin"
LOG_FILE = f"/var/log/{LOGGER_NAME}"
os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(LOGGER_NAME)
logger.setLevel(logging.INFO)
if not logger.handlers:
    try:
        os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
        fh = RotatingFileHandler(LOG_FILE, maxBytes=10*1024*1024, backupCount=5, encoding="utf-8")
    except Exception as e:
        fallback = f"/tmp/{LOGGER_NAME}.log"
        print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å {LOG_FILE} –¥–ª—è –∑–∞–ø–∏—Å–∏ ({e}). –ü–∏—à—É –≤ {fallback}")
        fh = RotatingFileHandler(fallback, maxBytes=10*1024*1024, backupCount=5, encoding="utf-8")
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)
    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    logger.addHandler(sh)
logger.propagate = False

# ================== AUTH ==================
load_dotenv()
TGSTAT_API_KEY = os.getenv("TGSTAT_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY
PEGASUS_API_KEY = os.getenv("PEGASUS_API_KEY")

client1 = OpenAI(api_key=OPENAI_API_KEY)
client2 = TwelveLabs(api_key=PEGASUS_API_KEY)

scope = ['https://spreadsheets.google.com/feeds',
         'https://www.googleapis.com/auth/drive']
creds = ServiceAccountCredentials.from_json_keyfile_name('creds.json', scope)
gs_client = gspread.authorize(creds)

def get_or_create_worksheet(spreadsheet_name, title, rows=100, cols=20):
    try:
        return spreadsheet_name.worksheet(title)
    except gspread.exceptions.WorksheetNotFound:
        logger.warning(f"‚ö†Ô∏è –õ–∏—Å—Ç '{title}' –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—é –Ω–æ–≤—ã–π...")
        return spreadsheet_name.add_worksheet(title=title, rows=rows, cols=cols)

# ================== GLOBAL VARIABLES ==================   
admin_spreadsheet = gs_client.open(ADMIN_SPREADSHEET_NAME)
admin_main = get_or_create_worksheet(admin_spreadsheet, MAIN)
admin_log = get_or_create_worksheet(admin_spreadsheet, LOG)

# ================== FUNCTIONS ==================
def get_channel_info(channel_id):
    url = URL_1
    params = {
        'token': TGSTAT_API_KEY,
        'channelId': channel_id
    }

    response = requests.get(url, params=params, timeout=15).json()
    if response.get("status", "") == "error":
        raise Exception(response['error'])
    ch = response.get("response", {})
    return {
        '–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞': ch.get("title", ""),
        'link': f"https://t.me/{ch.get('username', '')}" if ch.get("username") else channel_id,
        'ID': ch.get("id", ""),
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤': ch.get("participants_count", 0)
    }


def extract_channels_from_sheet(channels_worksheet):
    all_data = channels_worksheet.get_all_values()
    headers = all_data[0]
    data = all_data[1:]
    link_col_index = headers.index("link") # TODO: make it a variable
    channels_list = []
    for row in data:
        if link_col_index < len(row):
            link = row[link_col_index].strip()
            if link:
                channels_list.append(link)
    return channels_list


def save_to_sheet_channels(data, worksheet):
    header = ["–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞", "link", "ID", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"]
    rows = [[ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'], ch['link'], ch['ID'], ch["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"]] for ch in data]
    worksheet.clear()
    worksheet.append_row(header, value_input_option='RAW')
    worksheet.append_rows(rows, value_input_option='RAW')


def get_top_posts(channel_id, days_back, limit=50):
    url = URL_2
    date_to = datetime.today()
    date_from = date_to - timedelta(days=days_back)
    params = {
        'token': TGSTAT_API_KEY,
        'channelId': channel_id,
        'limit': limit,
        'startDate': date_from.strftime('%Y-%m-%d'),
        'endDate': date_to.strftime('%Y-%m-%d'),
        'extended': 1
    }
    response = requests.get(url, params=params, timeout=15)
    try:
        return response.json().get("response", {}).get("items", [])
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–ª—è channel {channel_id}: {e}")


def transform_to_normal_date(timestamp):
    try:
        dt = datetime.fromtimestamp(int(timestamp))
        return dt.strftime("%d.%m.%Y"), dt.strftime("%H:%M")
    except:
        return "", ""


def fetch_post_stats(post_link):
    url = URL_3
    parsed_url = urlparse(post_link)
    path_parts = parsed_url.path.split('/')
    if len(path_parts) < 3:
        return None
    params = {"token": TGSTAT_API_KEY, "postId": post_link}
    try:
        response = requests.get(url, params=params, timeout=15)
        data = response.json()
        if data.get("status") == "ok":
            return data["response"]
    except Exception as e:
        raise Exception(f"Exception for post {post_link}: {str(e)}")


def calculate_engagement(views, reactions, comments, forwards):
    return round((reactions + forwards + comments) / views * 100, 2) if views > 0 else 0


def extract_top_posts(company_id: int, company_name: str, channels_data, days_back, top_n):
    final_rows = []
    os.makedirs("extracted_data", exist_ok=True)
    
    all_posts = []
    all_stats = []
    
    for ch in channels_data:
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–Ω–∞–ª: {ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞']}")
        channel_id = ch['ID']
        try:
            posts = get_top_posts(channel_id, days_back)
            if not posts:
                # Warning
                raise Exception(f"–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –≤ –∫–∞–Ω–∞–ª–µ")
                
            # Collect posts for JSON
            all_posts.extend([{
                "channel_id": channel_id,
                "channel_name": ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'],
                "post": post,
                "timestamp": datetime.now().isoformat()
            } for post in posts])
            
            channel_posts = []
            for post in posts:
                text = post.get("text", "")
                if len(text.strip()) == 0:
                    continue
                post_link = post.get("link", "")
                if not post_link:
                    continue
                stats = fetch_post_stats(post_link)
                if not stats:
                    continue
                    
                # Collect stats for JSON
                all_stats.append({
                    "channel_id": channel_id,
                    "channel_name": ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'],
                    "post_link": post_link,
                    "stats": stats,
                    "timestamp": datetime.now().isoformat()
                })
                
                views = stats.get("viewsCount", 0)
                reactions = stats.get("reactionsCount", 0)
                comments = stats.get("commentsCount", 0)
                forwards = stats.get("forwardsCount", 0)
                engagement = calculate_engagement(
                    views, reactions, comments, forwards)
                post["engagement"] = engagement
                post["views"] = views
                post["reactions"] = reactions
                post["comments"] = comments
                post["forwards"] = forwards
                post["channel_info"] = ch
                post["processed_stats"] = stats
                channel_posts.append(post)
                
            top_channel_posts = sorted(
                channel_posts, key=lambda x: x["engagement"], reverse=True
            )[:top_n]
            
            for post in top_channel_posts:
                text = post.get("text", "")
                views = post.get("views", 0)
                reactions = post.get("reactions", 0)
                comments = post.get("comments", 0)
                forwards = post.get("forwards", 0)
                engagement = post.get("engagement", 0)
                post_link = post.get("link", "")
                media = post.get("media", {})
                file_url = media.get("file_url", "")
                video_link = file_url if file_url and file_url.endswith(
                    ".mp4") else ""
                date_only, time_only = transform_to_normal_date(
                    post.get("date", ""))
                post_length = len(text) if text else 0
                row = [
                    ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'],
                    ch["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"],
                    text,
                    post_link,
                    video_link,
                    date_only,
                    time_only,
                    post_length,
                    views,
                    reactions,
                    comments,
                    forwards,
                    engagement
                ]
                final_rows.append(row)
                
        except Exception as e:
            # Warning
            admin_log.insert_row([company_id, company_name, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {channel_id}: {e}", datetime.today().isoformat()], 2)
    
    # Save posts to channels_stats.json
    with open("extracted_data/channels_stats.json", "w", encoding="utf-8") as f:
        json.dump(all_posts, f, ensure_ascii=False, indent=2)
    
    # Save stats to posts_stats.json
    with open("extracted_data/posts_stats.json", "w", encoding="utf-8") as f:
        json.dump(all_stats, f, ensure_ascii=False, indent=2)
    
    return final_rows


def translate_into_russian(text):
    prompt = f"""
    –ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ –ø—Ä–∏—à–ª–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
    \"{text}\"
    """
    response = client1.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.8
    )
    return response.choices[0].message.content


def extract_json_from_response(content):
    """Extract JSON from markdown-wrapped content"""
    match = re.search(r"```json\s*(\{.*?\})\s*```", content, re.DOTALL)
    try:
        if match:
            json_str = match.group(1)
            return json.loads(json_str)
        else:
            return json.loads(content)
    except json.JSONDecodeError as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON: {e}")


def generate_index_name(url: str) -> str:
    """Generate unique index name based on video URL"""
    parsed = urlparse(url)
    basename = os.path.basename(parsed.path)
    name_hash = hashlib.md5(url.encode()).hexdigest()[:6]
    return f"video-index-{basename}-{name_hash}"


def get_or_create_index(name: str):
    """Create the index (only if not exists)"""
    existing = client2.indexes.list()
    for idx in existing:
        if idx.index_name == name:
            logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å: {idx.index_name}")
            return idx

    models = [{"model_name": "pegasus1.2", "model_options": ["visual", "audio"]}]
    index = client2.indexes.create(index_name=name, models=models)
    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: id={index.id}")
    return index

# TODO: remove function
def download_video(url: str) -> str:
    """Download video from URL to temp file"""
    logger.info("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∏–¥–µ–æ...")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp_file:
        video_path = tmp_file.name
        response = requests.get(url, stream=True, timeout=60)
        response.raise_for_status()
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                tmp_file.write(chunk)
    logger.info(f"üìÅ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ø–∞–ø–∫—É: {video_path}")
    return video_path


def transcribe_video(url: str) -> str:
    """Transcribe and summarize video"""
    if not url or not url.strip():
        return ""

    try:
        # video_path = download_video(url)
        index_name = generate_index_name(url)
        index = get_or_create_index(index_name)

        # with open(video_path, "rb") as video_file:
        task = client2.tasks.create(index_id=index.id, video_url=url)
        logger.info(f"üöÄ Task started: id={task.id}, video_id={task.video_id}")

        def on_task_update(task: TasksRetrieveResponse):
            logger.info(f"‚è≥ Status = {task.status}")

        task = client2.tasks.wait_for_done(task_id=task.id, callback=on_task_update)

        if task.status != "ready":
            raise RuntimeError(f"Indexing failed with status: {task.status}")


        res = client2.summarize(video_id=task.video_id,
                               type="summary", prompt=PEGASUS_SYS_ROLE)

        # if os.path.exists(video_path):
        #     os.remove(video_path)

        return res.summary
    except ApiError as e:
        error_body = getattr(e, 'body', {})
        if isinstance(error_body, dict):
            raise Exception(f"–û—à–∏–±–∫–∞ TwelveLabs API: {error_body}")
        raise Exception(f"–û—à–∏–±–∫–∞ TwelveLabs API: {e}")
    except TooManyRequestsError as e:
        raise Exception("–û—à–∏–±–∫–∞ TwelveLabs API: –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤")
    except Exception as e:
        # if 'video_path' in locals() and os.path.exists(video_path):
        #     os.remove(video_path)
        raise


def rewrite_post_into_blocks(post_text):
    """Analyze post and return structured data"""
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π Telegram-–ø–æ—Å—Ç –∏ –æ—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ –ø–æ –ø–æ–ª—è–º:
    - tema: —Ç–µ–º–∞ –ø–æ—Å—Ç–∞ (–∫–æ—Ä–æ—Ç–∫–æ)
    - format: —Ñ–æ—Ä–º–∞—Ç (—Ç–µ–∫—Å—Ç / –≤–∏–¥–µ–æ / –∫–∞—Ä—É—Å–µ–ª—å / –æ–ø—Ä–æ—Å –∏ —Ç.–ø.)
    - length: –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    - style: —Å–µ—Ä—å—ë–∑–Ω—ã–π / —é–º–æ—Ä–Ω–æ–π / —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π / —Å—Ç–æ—Ä–∏—Ç–µ–ª–ª–∏–Ω–≥ –∏ —Ç.–ø.
    - cta: –∫–∞–∫–æ–π –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é –µ—Å—Ç—å, –∏–ª–∏ "–Ω–µ—Ç", –µ—Å–ª–∏ –µ—Å—Ç—å, —Ç–æ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å
    - zagolovok_5_slov: —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ 5 —Å–ª–æ–≤
    - zagolovok_len: –¥–ª–∏–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    - fact: –µ—Å—Ç—å –ª–∏ –Ω–∞—É—á–Ω—ã–π —Ñ–∞–∫—Ç –∏–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: –¥–∞/–Ω–µ—Ç
    - benefit: –µ—Å—Ç—å –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø–æ–ª—å–∑–∞ –∏–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –¥–∞/–Ω–µ—Ç
    - comment_call: –µ—Å—Ç—å –ª–∏ –ø—Ä–∏–∑—ã–≤ –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å: –¥–∞/–Ω–µ—Ç
    - insight: –∫—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥, –≤ —á—ë–º —Å–∏–ª–∞ –ø–æ—Å—Ç–∞
    - filter: –æ–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ—Å—Ç –õ–∏—á–Ω—ã–º –∏–ª–∏ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º. 
      `–õ–∏—á–Ω–æ–µ` ‚Äî –ø–æ—Å—Ç—ã –æ –ª–∏—á–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö, –ª–∏—á–Ω—ã—Ö –≤–µ—â–∞—Ö, —Å–æ–±—ã—Ç–∏—è—Ö, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Å–µ–ª—å—Å–∫–∏–º —Ö–æ–∑—è–π—Å—Ç–≤–æ–º.
      `–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ` ‚Äî –ø–æ—Å—Ç—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Å–µ–ª—å—Å–∫–∏–º —Ö–æ–∑—è–π—Å—Ç–≤–æ–º, –∫–æ—Ä–º–∞–º–∏, –∂–∏–≤–æ—Ç–Ω–æ–≤–æ–¥—Å—Ç–≤–æ–º, —Å–æ–≤–µ—Ç–∞–º–∏ –¥–ª—è —Ñ–µ—Ä–º–µ—Ä–æ–≤.
    –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞:
    \"\"\"{post_text}\"\"\"
    """
    response = client1.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": OPENAI_SYS_ROLE},
                    {"role": "user", "content": prompt}],
        temperature=0.4
    )
    response_text = response.choices[0].message.content

    return extract_json_from_response(response_text)


def rewrite_post_with_context(post_text, context):
    """Rewrite post with company context"""
    prompt = f"""
    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
    –ù–∏–∂–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç –∏–∑ Telegram:
    \"{post_text}\"
    –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ–∑–¥–∞–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π Telegram-–ø–æ—Å—Ç –¥–ª—è –ü—Ä–æ—Ñ–ö–æ—Ä–º.
    –°–æ—Ö—Ä–∞–Ω–∏ –∏–¥–µ—é –∏ –ø–æ–ª—å–∑—É, –Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—à–∏ —Ç–µ–∫—Å—Ç –ø–æ–¥ —Å—Ç–∏–ª—å –ü—Ä–æ—Ñ–ö–æ—Ä–º.
    –ù–µ —É–ø–æ–º–∏–Ω–∞–π —á—É–∂–∏–µ –±—Ä–µ–Ω–¥—ã. –ü–∏—à–∏ —è—Å–Ω–æ, —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ –∏ –ø–æ –¥–µ–ª—É. –û–±—ä—ë–º ‚Äî –¥–æ 2049 —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏.
    """
    response = client1.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": OPENAI_SYS_ROLE},
                    {"role": "user", "content": prompt}],
        temperature=0.8
    )

    return response.choices[0].message.content


def create_video_suggestion(transcription, company_context):
    """Create video suggestion based on transcription and context"""
    if not transcription or transcription.startswith("Error:"):
        return ""

    prompt = f"""
    –ö–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–º–ø–∞–Ω–∏–∏: {company_context}
    
    –ù–∏–∂–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Å–∫—Ä–∏–ø—Ç –≤–∏–¥–µ–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞:
    \"{transcription}\"
    
    –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Å–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Å—ä–µ–º–∫–∏ –ø–æ—Ö–æ–∂–µ–≥–æ –≤–∏–¥–µ–æ –¥–ª—è –Ω–∞—à–µ–π –∫–æ–º–ø–∞–Ω–∏–∏.
    –í–∫–ª—é—á–∏:
    1. –ê–¥–∞–ø—Ç–∞—Ü–∏—é —Å—Ü–µ–Ω–∞—Ä–∏—è –ø–æ–¥ –Ω–∞—à –±—Ä–µ–Ω–¥ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
    2. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å—ä–µ–º–∫–µ
    3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ª–æ–∫–∞—Ü–∏–∏ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—É
    4. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É/—Ä–µ—á–∏
    5. –ò–¥–µ–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –∏–ª–∏ –≥—Ä–∞—Ñ–∏–∫–∏
    
    –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É–π –ø–æ–¥ –Ω–∞—à —Å—Ç–∏–ª—å –∏ –∞—É–¥–∏—Ç–æ—Ä–∏—é.
    """

    response = client1.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "–¢—ã –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ-–∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥ –±—Ä–µ–Ω–¥ –∫–æ–º–ø–∞–Ω–∏–∏."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content


def complete_ai_analysis_for_sheet(company_id: int, company_name: str, company_context: str, post_num: int, worksheet):
    """Complete AI analysis for all posts in the sheet including video processing"""
    headers = worksheet.row_values(1)
    new_data = worksheet.get_values(f"2:{post_num + 1}")
    
    post_text_col = headers.index("–ü–æ—Å—Ç - –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞")
    video_url_col = headers.index(
        "–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ") if "–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ" in headers else -1

    ai_columns = [
        "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –ø–æ—Å—Ç—É", "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –≤–∏–¥–µ–æ", "–¢–µ–º–∞ –ø–æ—Å—Ç–∞", "–§–æ—Ä–º–∞—Ç",
        "–°—Ç–∏–ª—å", "CTA", "–ó–∞–≥–æ–ª–æ–≤–æ–∫", "–î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞",
        "‚úÖ –ù–∞—É—á–Ω—ã–π —Ñ–∞–∫—Ç/–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "‚úÖ –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø–æ–ª—å–∑–∞ (–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å)",
        "‚úÖ –ü—Ä–∏–∑—ã–≤ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å", "–ò–Ω—Å–∞–π—Ç/–∑–∞–º–µ—Ç–∫–∞", "–§–∏–ª—å—Ç—Ä"
    ]

    for col in ai_columns:
        if col not in headers:
            headers.append(col)

    if len(headers) > worksheet.col_count:
        worksheet.add_cols(len(headers) - worksheet.col_count)

    worksheet.update(range_name="1:1", values=[headers])

    admin_log.insert_row([company_id, company_name, f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {post_num} —Å—Ç—Ä–æ–∫ —Å –ø–æ–ª–Ω—ã–º AI –∞–Ω–∞–ª–∏–∑–æ–º...", datetime.today().isoformat()], 2)

    enhanced_rows = []
    for i, row in enumerate(new_data):
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É {i+1}...")

        while len(row) < len(headers):
            row.append("")

        post_text = row[post_text_col] if post_text_col < len(row) else ""
        video_url = row[video_url_col] if video_url_col >= 0 and video_url_col < len(
            row) else ""

        if not post_text.strip():
            enhanced_rows.append(row)
            continue
        
        start_time = time.time()
        analysis = rewrite_post_into_blocks(post_text)
        end_time = time.time()

        logger.info(f"–ü–æ—Å—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        start_time = time.time()
        rewritten_post = rewrite_post_with_context(post_text, company_context)
        end_time = time.time()

        logger.info(f"–ü–æ—Å—Ç –ø–µ—Ä–µ–ø–∏—Å–∞–Ω –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")

        # Video processing
        video_suggestion = ""
        if video_url.strip():
            logger.info(f"üé• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ: {video_url}")
            try:
                start_time = time.time()
                transcription = transcribe_video(video_url.strip())
                end_time = time.time()

                logger.info(f"–í–∏–¥–µ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω–æ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")

                if transcription:
                    start_time = time.time()
                    translated_transcription = translate_into_russian(
                        transcription)
                    end_time = time.time()

                    logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø–µ—Ä–µ–≤–µ–¥–µ–Ω –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")

                    start_time = time.time()
                    video_suggestion = create_video_suggestion(
                        translated_transcription, company_context)
                    end_time = time.time()

                    logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –Ω–æ–≤—ã–π —Å—é–∂–µ—Ç –≤–∏–¥–µ–æ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
            except Exception as e:
                admin_log.insert_row([company_id, company_name, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ –≤ –ø–æ—Å—Ç–µ {i+1}: {e}", datetime.today().isoformat()], 2)
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ –≤ –ø–æ—Å—Ç–µ {i+1}: {e}")

        # Update row with all AI data
        col_mapping = {
            "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –ø–æ—Å—Ç—É": rewritten_post,
            "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –≤–∏–¥–µ–æ": video_suggestion,
            "–¢–µ–º–∞ –ø–æ—Å—Ç–∞": analysis.get("tema", ""),
            "–§–æ—Ä–º–∞—Ç": analysis.get("format", ""),
            "–°—Ç–∏–ª—å": analysis.get("style", ""),
            "CTA": analysis.get("cta", ""),
            "–ó–∞–≥–æ–ª–æ–≤–æ–∫": analysis.get("zagolovok_5_slov", ""),
            "–î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞": analysis.get("zagolovok_len", 0),
            "‚úÖ –ù–∞—É—á–Ω—ã–π —Ñ–∞–∫—Ç/–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ": analysis.get("fact", ""),
            "‚úÖ –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø–æ–ª—å–∑–∞ (–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å)": analysis.get("benefit", ""),
            "‚úÖ –ü—Ä–∏–∑—ã–≤ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å": analysis.get("comment_call", ""),
            "–ò–Ω—Å–∞–π—Ç/–∑–∞–º–µ—Ç–∫–∞": analysis.get("insight", ""),
            "–§–∏–ª—å—Ç—Ä": analysis.get("filter", "")
        }

        for col_name, value in col_mapping.items():
            if col_name in headers:
                col_idx = headers.index(col_name)
                row[col_idx] = value

        enhanced_rows.append(row)
        logger.info(f"  ‚úÖ –°—Ç—Ä–æ–∫–∞ {i+1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")

    worksheet.update(range_name=f"2:{post_num+1}", values=enhanced_rows)

    logger.info(f"‚úÖ –ü–æ–ª–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏ {company_name} c id {company_id}")
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(enhanced_rows)}")

async def extract_context(spreadsheet):
    try:
        worksheet = spreadsheet.worksheet(PROFILE)
    except gspread.exceptions.WorksheetNotFound:
        raise Exception(f"–ù–µ—Ç –ª–∏—Å—Ç–∞ {PROFILE}")

    top_left_cell = worksheet.cell(1, 1).value
    if not top_left_cell.strip():
        raise Exception(f"–ü—Ä–æ—Ñ–∏–ª—å –∫–æ–º–ø–∞–Ω–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–∫–∞–∑–∞–Ω –Ω–∞ –ª–∏—Å—Ç–µ {PROFILE} –≤ —è—á–µ–π–∫–µ A1")
    
    return top_left_cell

# ------------------ RUN ------------------
async def process_table(company_id: int, company_name: str, company_url: str, days_back=60):
    logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∫–ª–∏–µ–Ω—Ç–∞ {company_name} c id {company_id}...")
    try: 
        spreadsheet = gs_client.open_by_url(company_url)
    except:
        raise Exception("–ù–µ–≤–µ—Ä–Ω—ã–π URL")
    company_context = await extract_context(spreadsheet)
    # TODO: what if table not exist
    channels_sheet = get_or_create_worksheet(spreadsheet, CHANNELS)
    # TODO: what if table not exist
    suggestions_sheet = get_or_create_worksheet(spreadsheet, SUGGESTIONS)
    # TODO: should always update?
    suggestions_headers = [
        "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤",
        "–ü–æ—Å—Ç - –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞",
        "–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å—Ç",
        "–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ",
        "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
        "–í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
        "–î–ª–∏–Ω–Ω–∞ –ø–æ—Å—Ç–∞",
        "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã",
        "–†–µ–∞–∫—Ü–∏–∏",
        "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏",
        "–†–µ–ø–æ—Å—Ç—ã",
        "–í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å"
    ]
    suggestions_sheet.update(range_name='1:1', values=[suggestions_headers])

    # --- –°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞–Ω–∞–ª–∞—Ö ---
    raw_channels = extract_channels_from_sheet(channels_sheet)
    channel_infos = []
    for ch in raw_channels:
        try:
            channel_id = ch.strip()
            info = get_channel_info(channel_id)
            if info:
                channel_infos.append(info)
        except Exception as e:
            # Warning
            admin_log.insert_row([company_id, company_name, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {channel_id}: {e}", datetime.today().isoformat()], 2)

    if not channel_infos:
        raise Exception("–ö–∞–Ω–∞–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Google Sheets
    save_to_sheet_channels(channel_infos, channels_sheet)

    # --- –õ–æ–≥–∏—Ä—É–µ–º ---
    admin_log.insert_row([company_id, company_name, f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(channel_infos)} –∫–∞–Ω–∞–ª–æ–≤", datetime.today().isoformat()], 2)

    # --- –°–±–æ—Ä –ø–æ—Å—Ç–æ–≤ ---
    channels_data = channels_sheet.get_all_records()
    data = [ch for ch in channels_data if ch.get('ID') and ch.get('–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞')]
    rows = extract_top_posts(company_id, company_name, data, days_back, top_n=10)

    logger.info(f"–í—Å–µ–≥–æ –≤—ã–±—Ä–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(rows)}")

    if not rows:
        raise Exception("–ù–µ—Ç –ø–æ—Å—Ç–æ–≤")

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Google Sheets ---
    suggestions_sheet.insert_rows(rows, value_input_option='RAW', row=2)

    # --- –õ–æ–≥–∏—Ä—É–µ–º ---
    admin_log.insert_row([company_id, company_name, f"–°–æ–±—Ä–∞–Ω–æ {len(rows)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", datetime.today().isoformat()], 2)

    complete_ai_analysis_for_sheet(company_id, company_name, company_context, len(rows), suggestions_sheet)
    
    admin_log.insert_row([company_id, company_name, "AI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω", datetime.today().isoformat()], 2)
 
    

def get_col_idx(col_name, headers):
    try:
        col_idx = headers.index(col_name.lower())
    except ValueError:
        raise Exception("–ö–æ–ª–æ–Ω–∫–∞ '{col_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    return col_idx

# async def create_client(i: int, company_id: int, company_name: str, company_url: str, headers):
#     created_col = get_col_idx('Created', headers)
#     updated_col = get_col_idx('Updated', headers)
#     status_col = get_col_idx('Status', headers)

#     admin_main.update_cell(i+2, created_col+1, datetime.today().strftime('%Y-%m-%d'))
    
#     await process_table(company_id, company_name, company_url)

#     admin_main.update_cell(i+2, status_col+1, 'In progress')
    # admin_main.update_cell(i+2, updated_col+1, datetime.today().strftime('%Y-%m-%d'))

def main():
    all_data = admin_main.get_all_values()
    if not all_data:
        logger.error(f"–õ–∏—Å—Ç '{MAIN}' –ø—É—Å—Ç–æ–π")
        return

    headers = [x.lower() for x in all_data[0]]
    rows = all_data[1:]
    
    try:
        id_col = get_col_idx('id', headers)
        name_col = get_col_idx('Name', headers)
        url_col = get_col_idx('URL', headers)
        status_col = get_col_idx('Scheduler Status', headers)
        processing_col = get_col_idx('Processing', headers)

        clients_to_process = []

        for i, row in enumerate(rows):
            client_id = row[id_col].strip() if id_col < len(row) else ''
            client_name = row[name_col].strip() if name_col < len(row) else ''
            client_url = row[url_col].strip() if url_col < len(row) else ''
            client_status = row[status_col].strip() if status_col < len(row) else ''
            

            if client_status == 'Start' or client_status == 'In progress':
                if not client_id.isdigit():
                    admin_log.insert_row([client_id, client_name, f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π id '{client_id}' –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}", datetime.today().isoformat()], 2)
                    logger.error(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π id '{client_id}' –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}")
                    admin_main.update_cell(i+2, processing_col+1, '–û—à–∏–±–∫–∞')
                client_id = int(client_id)
                if client_name and client_url:
                    clients_to_process.append((i, client_id, client_name, client_url, client_status))
                    admin_main.update_cell(i+2, processing_col+1, '–í –æ–∂–∏–¥–∞–Ω–∏–∏...')
                else:
                    admin_log.insert_row([client_id, client_name, f"–ù–µ —É–∫–∞–∑–∞–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}", datetime.today().isoformat()], 2)
                    logger.error(f"–ù–µ —É–∫–∞–∑–∞–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}")
                    admin_main.update_cell(i+2, processing_col+1, '–û—à–∏–±–∫–∞')
        
        for client in clients_to_process:
            i, client_id, client_name, client_url, client_status = client 
            admin_main.update_cell(i+2, processing_col+1, '–í –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–∏')
            try:
                if client_status == 'Start':
                    asyncio.run(process_table(client_id, client_name, client_url))
                    admin_main.update_cell(i+2, status_col+1, 'In progress')
                else:
                    asyncio.run(process_table(client_id, client_name, client_url, 7))
                admin_main.update_cell(i+2, processing_col+1, '–ì–æ—Ç–æ–≤–æ')
            except PermissionDeniedError:
                admin_log.insert_row([client_id, client_name, "–û—à–∏–±–∫–∞ OpenAI API: –≤–∫–ª—é—á–∏—Ç–µ VPN", datetime.today().isoformat()], 2)
                logger.error(f"–û—à–∏–±–∫–∞ OpenAI API: –≤–∫–ª—é—á–∏—Ç–µ VPN")
                admin_main.update_cell(i+2, processing_col+1, '–û—à–∏–±–∫–∞')
            except RateLimitError:
                admin_log.insert_row([client_id, client_name, "–û—à–∏–±–∫–∞ OpenAI API: –∏—Å—á–µ—Ä–ø–∞–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤", datetime.today().isoformat()], 2)
                logger.error(f"–û—à–∏–±–∫–∞ OpenAI API: –∏—Å—á–µ—Ä–ø–∞–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤")
                admin_main.update_cell(i+2, processing_col+1, '–û—à–∏–±–∫–∞')
            except AuthenticationError:
                admin_log.insert_row([client_id, client_name, "–û—à–∏–±–∫–∞ OpenAI API: –æ—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏", datetime.today().isoformat()], 2)
                logger.error(f"–û—à–∏–±–∫–∞ OpenAI API: –æ—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                admin_main.update_cell(i+2, processing_col+1, '–û—à–∏–±–∫–∞')
            except Exception as e:
                admin_log.insert_row([client_id, client_name, str(e), datetime.today().isoformat()], 2)
                logger.error(str(e))
                admin_main.update_cell(i+2, processing_col+1, '–û—à–∏–±–∫–∞')
        
        for client in clients_to_process:
            i, client_id, client_name, client_url, client_status = client
            if admin_main.cell(i+2, processing_col+1).value == '–ì–æ—Ç–æ–≤–æ':
                admin_main.update_cell(i+2, processing_col+1, '')
    
    except Exception as e:
        logger.error(str(e))
        

if __name__ == "__main__":
    main()
