""" –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–ø–¥–µ–π—Ç–∞ —Å—Ç–µ–Ω–¥–∞ –Ω–∞ –≥—É–≥–ª –¥–∏—Å–∫–µ, —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∏–º–µ—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞–Ω–∞–ª—ã –Ω–∞ –ª–∏—Å—Ç–µ - –ö–∞–Ω–∞–ª—ã
"""
import os
import time
import re
import json
import hashlib
import tempfile
from datetime import datetime, timedelta
from urllib.parse import urlparse
import asyncio


import gspread
from oauth2client.service_account import ServiceAccountCredentials
import requests
from dotenv import load_dotenv
import openai
from openai import OpenAI
from twelvelabs import TwelveLabs
from twelvelabs.tasks import TasksRetrieveResponse

# ================== CONSTANTS ==================
URL_1 = "https://api.tgstat.ru/channels/get"
URL_2 = "https://api.tgstat.ru/channels/posts"
URL_3 = "https://api.tgstat.ru/posts/stat"

with open('prompts/openai_sys_role.txt', 'r', encoding='utf-8') as f2:
    OPENAI_SYS_ROLE = f2.read().strip()
with open('prompts/pegasus_sys_role.txt', 'r', encoding='utf-8') as f3:
    PEGASUS_SYS_ROLE = f3.read().strip()
with open('prompts/headers.json', 'r', encoding='utf-8') as f4:
    final_headers = json.load(f4)

ADMIN_SPREADSHEET_NAME: str = "Sellebra TGstat (admin)"
CHANNELS: str = '–ö–∞–Ω–∞–ª—ã'
SUGGESTIONS: str ='–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏'
MAIN: str = 'Main'
LOG: str = 'Log'
# ================== AUTH ==================
load_dotenv()
TGSTAT_API_KEY = os.getenv("TGSTAT_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai.api_key = OPENAI_API_KEY
PEGASUS_API_KEY = os.getenv("PEGASUS_API_KEY")

client1 = OpenAI(api_key=OPENAI_API_KEY)
client2 = TwelveLabs(api_key=PEGASUS_API_KEY)

scope = ['https://spreadsheets.google.com/feeds',
         'https://www.googleapis.com/auth/drive']
creds = ServiceAccountCredentials.from_json_keyfile_name('creds.json', scope)
gs_client = gspread.authorize(creds)

# ================== FUNCTIONS ==================
def get_or_create_worksheet(spreadsheet_name, title, rows=100, cols=20):
    try:
        return spreadsheet_name.worksheet(title)
    except gspread.exceptions.WorksheetNotFound:
        print(f"‚ö†Ô∏è –õ–∏—Å—Ç '{title}' –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—é –Ω–æ–≤—ã–π...")
        return spreadsheet_name.add_worksheet(title=title, rows=rows, cols=cols)

def get_channel_info(channel_id):
    url = URL_1
    params = {
        'token': TGSTAT_API_KEY,
        'channelId': channel_id
    }
    try:
        response = requests.get(url, params=params, timeout=15).json()
        if response.get("status", "") == "error":
            raise Exception(response['error'])
        ch = response.get("response", {})
        return {
            '–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞': ch.get("title", ""),
            'link': f"https://t.me/{ch.get('username', '')}" if ch.get("username") else channel_id,
            'ID': ch.get("id", ""),
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤': ch.get("participants_count", 0)
        }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {channel_id}: {e}")
        return None

def extract_channels_from_sheet(channels_worksheet):
    all_data = channels_worksheet.get_all_values()
    headers = all_data[0]
    data = all_data[1:]
    link_col_index = headers.index("link")
    channels_list = []
    for row in data:
        if link_col_index < len(row):
            link = row[link_col_index].strip()
            if link:
                channels_list.append(link)
    return channels_list

def save_to_sheet_channels(data, worksheet):
    header = ["–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞", "link", "ID", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"]
    rows = [[ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'], ch['link'], ch['ID'], ch["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"]] for ch in data]
    worksheet.clear()
    worksheet.append_row(header, value_input_option='RAW')
    worksheet.append_rows(rows, value_input_option='RAW')

def get_top_posts(channel_id, days_back, limit=50):
    url = URL_2
    date_to = datetime.today()
    date_from = date_to - timedelta(days=days_back)
    params = {
        'token': TGSTAT_API_KEY,
        'channelId': channel_id,
        'limit': limit,
        'startDate': date_from.strftime('%Y-%m-%d'),
        'endDate': date_to.strftime('%Y-%m-%d'),
        'extended': 1
    }
    response = requests.get(url, params=params, timeout=15)
    try:
        return response.json().get("response", {}).get("items", [])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–ª—è channel {channel_id}: {e}")
        return []

def transform_to_normal_date(timestamp):
    try:
        dt = datetime.fromtimestamp(int(timestamp))
        return dt.strftime("%d.%m.%Y"), dt.strftime("%H:%M")
    except:
        return "", ""

def fetch_post_stats(post_link):
    url = URL_3
    parsed_url = urlparse(post_link)
    path_parts = parsed_url.path.split('/')
    if len(path_parts) < 3:
        return None
    params = {"token": TGSTAT_API_KEY, "postId": post_link}
    try:
        response = requests.get(url, params=params, timeout=15)
        data = response.json()
        if data.get("status") == "ok":
            return data["response"]
    except Exception as e:
        print(f"Exception for post {post_link}: {str(e)}")
    return None

def calculate_engagement(views, reactions, comments, forwards):
    return round((reactions + forwards + comments) / views * 100, 2) if views > 0 else 0

def extract_top_posts(channels_data, days_back, top_n):
    final_rows = []
    os.makedirs("extracted_data", exist_ok=True)
    
    all_posts = []
    all_stats = []
    
    for ch in channels_data:
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–Ω–∞–ª: {ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞']}")
        channel_id = ch['ID']
        try:
            posts = get_top_posts(channel_id, days_back)
            if not posts:
                print("–ù–µ—Ç –ø–æ—Å—Ç–æ–≤.")
                continue
                
            # Collect posts for JSON
            all_posts.extend([{
                "channel_id": channel_id,
                "channel_name": ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'],
                "post": post,
                "timestamp": datetime.now().isoformat()
            } for post in posts])
            
            channel_posts = []
            for post in posts:
                text = post.get("text", "")
                if len(text.strip()) == 0:
                    continue
                post_link = post.get("link", "")
                if not post_link:
                    continue
                stats = fetch_post_stats(post_link)
                if not stats:
                    continue
                    
                # Collect stats for JSON
                all_stats.append({
                    "channel_id": channel_id,
                    "channel_name": ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'],
                    "post_link": post_link,
                    "stats": stats,
                    "timestamp": datetime.now().isoformat()
                })
                
                views = stats.get("viewsCount", 0)
                reactions = stats.get("reactionsCount", 0)
                comments = stats.get("commentsCount", 0)
                forwards = stats.get("forwardsCount", 0)
                engagement = calculate_engagement(
                    views, reactions, comments, forwards)
                post["engagement"] = engagement
                post["views"] = views
                post["reactions"] = reactions
                post["comments"] = comments
                post["forwards"] = forwards
                post["channel_info"] = ch
                post["processed_stats"] = stats
                channel_posts.append(post)
                
            top_channel_posts = sorted(
                channel_posts, key=lambda x: x["engagement"], reverse=True
            )[:top_n]
            
            for post in top_channel_posts:
                text = post.get("text", "")
                views = post.get("views", 0)
                reactions = post.get("reactions", 0)
                comments = post.get("comments", 0)
                forwards = post.get("forwards", 0)
                engagement = post.get("engagement", 0)
                post_link = post.get("link", "")
                media = post.get("media", {})
                file_url = media.get("file_url", "")
                video_link = file_url if file_url and file_url.endswith(
                    ".mp4") else ""
                date_only, time_only = transform_to_normal_date(
                    post.get("date", ""))
                post_length = len(text) if text else 0
                row = [
                    ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞'],
                    ch["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"],
                    text,
                    views,
                    post_link,
                    video_link,
                    "",
                    "",
                    date_only,
                    time_only,
                    "",
                    "",
                    post_length,
                    "",
                    "",
                    views,
                    reactions,
                    comments,
                    forwards,
                    "",
                    "",
                    "",
                    "",
                    "",
                    engagement,
                    "",
                    ""
                ]
                final_rows.append(row)
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {ch['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞']}: {e}")
    
    # Save posts to channels_stats.json
    with open("extracted_data/channels_stats.json", "w", encoding="utf-8") as f:
        json.dump(all_posts, f, ensure_ascii=False, indent=2)
    
    # Save stats to posts_stats.json
    with open("extracted_data/posts_stats.json", "w", encoding="utf-8") as f:
        json.dump(all_stats, f, ensure_ascii=False, indent=2)
    
    print(f"–í—Å–µ–≥–æ –≤—ã–±—Ä–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {len(final_rows)}")
    return final_rows

def save_to_sheet_suggestions(rows, worksheet):
    worksheet.clear()
    worksheet.append_row(final_headers, value_input_option='RAW')
    worksheet.append_rows(rows, value_input_option='RAW')
    return worksheet

def translate_into_russian(text):
    prompt = f"""
    –ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ –ø—Ä–∏—à–ª–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
    \"{text}\"
    """
    response = client1.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.8
    )
    return response.choices[0].message.content

def insert_ai_suggestions(rows, worksheet):
    header = [
        "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤",
        "–ü–æ—Å—Ç - –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞",
        "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤",
        "–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å—Ç",
        "–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ",
        "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –ø–æ—Å—Ç—É",
        "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –≤–∏–¥–µ–æ"
    ]
    worksheet.clear()
    worksheet.append_row(header, value_input_option='RAW')
    worksheet.append_rows(rows, value_input_option='RAW')
    
# def generate_suggestions(channels_sheet, suggestions_sheet):

#     records = channels_sheet.get_all_records()
#     updated_rows = []

#     for _, row in enumerate(records):
#         print(f"üîç –ê–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç–∞: {row['–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞']}")

#         text = row.get("–ü–æ—Å—Ç - –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞", "")
#         video_url = row.get("–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ", "")

#         # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞
#         try:
#             text_suggestion = rewrite_post_with_context(text)
#         except Exception as e:
#             print(f"‚ùå –û—à–∏–±–∫–∞ –≤ rewrite_post_with_context: {e}")
#             text_suggestion = ""
#             continue

#         # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ
#         try:
#             if video_url:
#                 video_suggestion = translate_into_russian(
#                     transcribe_video(video_url))
#             else:
#                 video_suggestion = ""
#         except Exception as e:
#             print(f"‚ùå –û—à–∏–±–∫–∞ –≤ transcribe_video: {e}")
#             video_suggestion = ""
#             continue

#         updated_row = [
#             row["–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞"],
#             row["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"],
#             text,
#             row["–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"],
#             row["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"],
#             row["–°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å—Ç"],
#             video_url,
#             text_suggestion,
#             video_suggestion
#         ]
#         updated_rows.append(updated_row)
#     insert_ai_suggestions(updated_rows, suggestions_sheet)

def extract_json_from_response(content):
    """Extract JSON from markdown-wrapped content"""
    match = re.search(r"```json\s*(\{.*?\})\s*```", content, re.DOTALL)
    if match:
        json_str = match.group(1)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON: {e}")
            return None
    else:
        try:
            return json.loads(content)
        except json.JSONDecodeError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –±–µ–∑ markdown: {e}")
            return None

def generate_index_name(url: str) -> str:
    """Generate unique index name based on video URL"""
    parsed = urlparse(url)
    basename = os.path.basename(parsed.path)
    name_hash = hashlib.md5(url.encode()).hexdigest()[:6]
    return f"video-index-{basename}-{name_hash}"

def get_or_create_index(name: str):
    """Create the index (only if not exists)"""
    existing = client2.index.list()
    for idx in existing:
        if idx.name == name:
            print(f"‚úÖ Using existing index: {idx.name}")
            return idx

    models = [{"name": "pegasus1.2", "options": ["visual", "audio"]}]
    index = client2.index.create(name=name, models=models)
    print(f"‚úÖ Index created: id={index.id}, name={index.name}")
    return index

def download_video(url: str) -> str:
    """Download video from URL to temp file"""
    print("üì• Downloading video...")
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp_file:
            video_path = tmp_file.name
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    tmp_file.write(chunk)
        print(f"üìÅ Video saved to: {video_path}")
        return video_path
    except Exception as e:
        print(f"‚ùå Error downloading video: {e}")
        raise


def transcribe_video(url: str) -> str:
    """Transcribe and summarize video"""
    if not url or not url.strip():
        return ""

    try:
        video_path = download_video(url)
        index_name = generate_index_name(url)
        index = get_or_create_index(index_name)

        task = client2.tasks.create(index_id=index.id, video_url=video_path)
        print(f"üöÄ Task started: id={task.id}, video_id={task.video_id}")

        def on_task_update(task: TasksRetrieveResponse):
            print(f"‚è≥ Status = {task.status}")

        task = client2.tasks.wait_for_done(task_id=task.id, callback=on_task_update)

        if task.status != "ready":
            raise RuntimeError(f"Indexing failed with status: {task.status}")


        res = client2.summarize(video_id=task.video_id,
                               type="summary", prompt=PEGASUS_SYS_ROLE)

        if os.path.exists(video_path):
            os.remove(video_path)

        return res.summary

    except Exception as e:
        print(f"‚ùå Error transcribing video {url}: {e}")
        if 'video_path' in locals() and os.path.exists(video_path):
            os.remove(video_path)
        return f"Error: {str(e)}"


def rewrite_post_into_blocks(post_text):
    """Analyze post and return structured data"""
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π Telegram-–ø–æ—Å—Ç –∏ –æ—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ –ø–æ –ø–æ–ª—è–º:
    - tema: —Ç–µ–º–∞ –ø–æ—Å—Ç–∞ (–∫–æ—Ä–æ—Ç–∫–æ)
    - format: —Ñ–æ—Ä–º–∞—Ç (—Ç–µ–∫—Å—Ç / –≤–∏–¥–µ–æ / –∫–∞—Ä—É—Å–µ–ª—å / –æ–ø—Ä–æ—Å –∏ —Ç.–ø.)
    - length: –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    - style: —Å–µ—Ä—å—ë–∑–Ω—ã–π / —é–º–æ—Ä–Ω–æ–π / —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π / —Å—Ç–æ—Ä–∏—Ç–µ–ª–ª–∏–Ω–≥ –∏ —Ç.–ø.
    - cta: –∫–∞–∫–æ–π –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é –µ—Å—Ç—å, –∏–ª–∏ "–Ω–µ—Ç", –µ—Å–ª–∏ –µ—Å—Ç—å, —Ç–æ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å
    - zagolovok_5_slov: —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ 5 —Å–ª–æ–≤
    - zagolovok_len: –¥–ª–∏–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    - fact: –µ—Å—Ç—å –ª–∏ –Ω–∞—É—á–Ω—ã–π —Ñ–∞–∫—Ç –∏–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: –¥–∞/–Ω–µ—Ç
    - benefit: –µ—Å—Ç—å –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø–æ–ª—å–∑–∞ –∏–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –¥–∞/–Ω–µ—Ç
    - comment_call: –µ—Å—Ç—å –ª–∏ –ø—Ä–∏–∑—ã–≤ –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å: –¥–∞/–Ω–µ—Ç
    - insight: –∫—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥, –≤ —á—ë–º —Å–∏–ª–∞ –ø–æ—Å—Ç–∞
    - filter: –æ–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ—Å—Ç –õ–∏—á–Ω—ã–º –∏–ª–∏ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º. 
      `–õ–∏—á–Ω–æ–µ` ‚Äî –ø–æ—Å—Ç—ã –æ –ª–∏—á–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è—Ö, –ª–∏—á–Ω—ã—Ö –≤–µ—â–∞—Ö, —Å–æ–±—ã—Ç–∏—è—Ö, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Å–µ–ª—å—Å–∫–∏–º —Ö–æ–∑—è–π—Å—Ç–≤–æ–º.
      `–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ` ‚Äî –ø–æ—Å—Ç—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Å–µ–ª—å—Å–∫–∏–º —Ö–æ–∑—è–π—Å—Ç–≤–æ–º, –∫–æ—Ä–º–∞–º–∏, –∂–∏–≤–æ—Ç–Ω–æ–≤–æ–¥—Å—Ç–≤–æ–º, —Å–æ–≤–µ—Ç–∞–º–∏ –¥–ª—è —Ñ–µ—Ä–º–µ—Ä–æ–≤.
    –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞:
    \"\"\"{post_text}\"\"\"
    """
    try:
        response = client1.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": OPENAI_SYS_ROLE},
                      {"role": "user", "content": prompt}],
            temperature=0.4
        )
        response_text = response.choices[0].message.content

        return extract_json_from_response(response_text) or {}
    except Exception as e:
        print(f"Error analyzing post: {e}")
        return {}


def rewrite_post_with_context(post_text, context):
    """Rewrite post with company context"""
    prompt = f"""
    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
    –ù–∏–∂–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ—Å—Ç –∏–∑ Telegram:
    \"{post_text}\"
    –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ–∑–¥–∞–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π Telegram-–ø–æ—Å—Ç –¥–ª—è –ü—Ä–æ—Ñ–ö–æ—Ä–º.
    –°–æ—Ö—Ä–∞–Ω–∏ –∏–¥–µ—é –∏ –ø–æ–ª—å–∑—É, –Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—à–∏ —Ç–µ–∫—Å—Ç –ø–æ–¥ —Å—Ç–∏–ª—å –ü—Ä–æ—Ñ–ö–æ—Ä–º.
    –ù–µ —É–ø–æ–º–∏–Ω–∞–π —á—É–∂–∏–µ –±—Ä–µ–Ω–¥—ã. –ü–∏—à–∏ —è—Å–Ω–æ, —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ –∏ –ø–æ –¥–µ–ª—É. –û–±—ä—ë–º ‚Äî –¥–æ 2049 —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏.
    """
    try:
        response = client1.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": OPENAI_SYS_ROLE},
                      {"role": "user", "content": prompt}],
            temperature=0.8
        )

        return response.choices[0].message.content
    except Exception as e:
        print(f"Error rewriting post: {e}")
        return ""


def create_video_suggestion(transcription):
    """Create video suggestion based on transcription and context"""
    if not transcription or transcription.startswith("Error:"):
        return ""

    prompt = f"""
    –ö–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–º–ø–∞–Ω–∏–∏: {CONTEXT}
    
    –ù–∏–∂–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Å–∫—Ä–∏–ø—Ç –≤–∏–¥–µ–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞:
    \"{transcription}\"
    
    –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Å–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —Å—ä–µ–º–∫–∏ –ø–æ—Ö–æ–∂–µ–≥–æ –≤–∏–¥–µ–æ –¥–ª—è –Ω–∞—à–µ–π –∫–æ–º–ø–∞–Ω–∏–∏.
    –í–∫–ª—é—á–∏:
    1. –ê–¥–∞–ø—Ç–∞—Ü–∏—é —Å—Ü–µ–Ω–∞—Ä–∏—è –ø–æ–¥ –Ω–∞—à –±—Ä–µ–Ω–¥ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
    2. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å—ä–µ–º–∫–µ
    3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ª–æ–∫–∞—Ü–∏–∏ –∏ —Ä–µ–∫–≤–∏–∑–∏—Ç—É
    4. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É/—Ä–µ—á–∏
    5. –ò–¥–µ–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –∏–ª–∏ –≥—Ä–∞—Ñ–∏–∫–∏
    
    –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É–π –ø–æ–¥ –Ω–∞—à —Å—Ç–∏–ª—å –∏ –∞—É–¥–∏—Ç–æ—Ä–∏—é.
    """

    try:
        response = client1.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "–¢—ã –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ-–∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥ –±—Ä–µ–Ω–¥ –∫–æ–º–ø–∞–Ω–∏–∏."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"‚ùå Error creating video suggestion: {e}")
        return f"–ë–∞–∑–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {transcription}"


def complete_ai_analysis_for_sheet(worksheet, company_context):
    """Complete AI analysis for all posts in the sheet including video processing"""
    try:
        all_data = worksheet.get_all_values()
        if not all_data:
            print("‚ùå –õ–∏—Å—Ç –ø—É—Å—Ç–æ–π")
            return

        headers = all_data[0]
        rows = all_data[1:]

        try:
            post_text_col = headers.index("–ü–æ—Å—Ç - –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞")
            video_url_col = headers.index(
                "–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ") if "–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ" in headers else -1
        except ValueError:
            print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ '–ü–æ—Å—Ç - –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return

        ai_columns = [
            "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –ø–æ—Å—Ç—É", "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –≤–∏–¥–µ–æ", "–¢–µ–º–∞ –ø–æ—Å—Ç–∞", "–§–æ—Ä–º–∞—Ç",
            "–°—Ç–∏–ª—å", "CTA", "–ó–∞–≥–æ–ª–æ–≤–æ–∫", "–î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞",
            "‚úÖ –ù–∞—É—á–Ω—ã–π —Ñ–∞–∫—Ç/–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "‚úÖ –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø–æ–ª—å–∑–∞ (–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å)",
            "‚úÖ –ü—Ä–∏–∑—ã–≤ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å", "–ò–Ω—Å–∞–π—Ç/–∑–∞–º–µ—Ç–∫–∞", "–§–∏–ª—å—Ç—Ä"
        ]

        for col in ai_columns:
            if col not in headers:
                headers.append(col)

        if len(headers) > worksheet.col_count:
            worksheet.add_cols(len(headers) - worksheet.col_count)

        worksheet.update("1:1", [headers])

        print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(rows)} —Å—Ç—Ä–æ–∫ —Å –ø–æ–ª–Ω—ã–º AI –∞–Ω–∞–ª–∏–∑–æ–º...")

        enhanced_rows = []
        for i, row in enumerate(rows, start=2):
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É {i}...")

            while len(row) < len(headers):
                row.append("")

            post_text = row[post_text_col] if post_text_col < len(row) else ""
            video_url = row[video_url_col] if video_url_col >= 0 and video_url_col < len(
                row) else ""

            if not post_text.strip():
                enhanced_rows.append(row)
                continue

            # AI Analysis for text
            print("üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞...")
            analysis = rewrite_post_into_blocks(post_text)
            rewritten_post = rewrite_post_with_context(post_text, company_context)

            # Video processing
            video_suggestion = ""
            if video_url.strip():
                print(f"üé• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ: {video_url}")
                try:
                    transcription = transcribe_video(video_url.strip())
                    if transcription and not transcription.startswith("Error:"):
                        translated_transcription = translate_into_russian(
                            transcription)
                        video_suggestion = create_video_suggestion(
                            translated_transcription)

                    else:
                        video_suggestion = "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"
                except Exception as e:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ: {e}")
                    video_suggestion = f"–û—à–∏–±–∫–∞: {str(e)}"

            # Update row with all AI data
            col_mapping = {
                "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –ø–æ—Å—Ç—É": rewritten_post,
                "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –≤–∏–¥–µ–æ": video_suggestion,
                "–¢–µ–º–∞ –ø–æ—Å—Ç–∞": analysis.get("tema", ""),
                "–§–æ—Ä–º–∞—Ç": analysis.get("format", ""),
                "–°—Ç–∏–ª—å": analysis.get("style", ""),
                "CTA": analysis.get("cta", ""),
                "–ó–∞–≥–æ–ª–æ–≤–æ–∫": analysis.get("zagolovok_5_slov", ""),
                "–î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞": analysis.get("zagolovok_len", 0),
                "‚úÖ –ù–∞—É—á–Ω—ã–π —Ñ–∞–∫—Ç/–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ": analysis.get("fact", ""),
                "‚úÖ –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø–æ–ª—å–∑–∞ (–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å)": analysis.get("benefit", ""),
                "‚úÖ –ü—Ä–∏–∑—ã–≤ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å": analysis.get("comment_call", ""),
                "–ò–Ω—Å–∞–π—Ç/–∑–∞–º–µ—Ç–∫–∞": analysis.get("insight", ""),
                "–§–∏–ª—å—Ç—Ä": analysis.get("filter", "")
            }

            for col_name, value in col_mapping.items():
                if col_name in headers:
                    col_idx = headers.index(col_name)
                    row[col_idx] = value

            enhanced_rows.append(row)
            print(f"  ‚úÖ –°—Ç—Ä–æ–∫–∞ {i} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")

        worksheet.update(f"2:{len(enhanced_rows)+1}", enhanced_rows)

        print("‚úÖ –ü–æ–ª–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è –ª–∏—Å—Ç–∞")
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(enhanced_rows)}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º AI –∞–Ω–∞–ª–∏–∑–µ –ª–∏—Å—Ç–∞ : {e}")

# ------------------ RUN ------------------
async def process_table(file_url: str, company_context: str, days_back=60):
    start_time = time.time()
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞...")

    spreadsheet = gs_client.open_by_url(file_url)
    channels_sheet = get_or_create_worksheet(spreadsheet, CHANNELS)
    suggestions_sheet = get_or_create_worksheet(spreadsheet, SUGGESTIONS)

    # --- –°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–∞–Ω–∞–ª–∞—Ö ---
    raw_channels = extract_channels_from_sheet(channels_sheet)
    channel_infos = []
    for ch in raw_channels:
        info = get_channel_info(ch.strip())
        if info:
            channel_infos.append(info)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Google Sheets
    save_to_sheet_channels(channel_infos, channels_sheet)

    # --- –°–±–æ—Ä –ø–æ—Å—Ç–æ–≤ ---
    channels_data = channels_sheet.get_all_records()
    data = [ch for ch in channels_data if ch.get('ID') and ch.get('–ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞')]
    rows = extract_top_posts(data, days_back, top_n=10)

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Google Sheets ---
    save_to_sheet_suggestions(rows, suggestions_sheet)
    complete_ai_analysis_for_sheet(suggestions_sheet, company_context)

    # --- –ó–∞—Å–µ—á–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ ---
    end_time = time.time()
    elapsed = end_time - start_time
    minutes, seconds = divmod(int(elapsed), 60)
    print(f"\nüéâ –í–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å AI –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {minutes} –º–∏–Ω {seconds} —Å–µ–∫")


async def create_client(i: int, client_name: str, client_url: str, admin_main, headers):
    id_col = headers.index('id')
    created_col = headers.index('created')
    updated_col = headers.index('updated')
    status_col = headers.index('status')
    context_col = headers.index('company context')

    # admin_main.update_cell(i+2, created_col+1, datetime.today().strftime('%Y-%m-%d'))
    
    company_id = admin_main.cell(i+2, id_col + 1).value
    company_context = admin_main.cell(i+2, context_col+1).value
    await process_table(client_url, company_context)

    # admin_main.update_cell(i+2, updated_col+1, datetime.today().strftime('%Y-%m-%d'))
    admin_main.update_cell(i+2, status_col+1, 'Start')

def main():
    admin_spreadsheet = gs_client.open(ADMIN_SPREADSHEET_NAME)
    admin_main = get_or_create_worksheet(admin_spreadsheet, MAIN)
    admin_log = get_or_create_worksheet(admin_spreadsheet, LOG)

    
    all_data = admin_main.get_all_values()
    if not all_data:
        print(f"‚ùå –õ–∏—Å—Ç '{MAIN}' –ø—É—Å—Ç–æ–π")
        return

    headers = [x.lower() for x in all_data[0]]
    rows = all_data[1:]
    
    try:
        name_col = headers.index("name")
    except ValueError:
        print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'Name' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    try:
        url_col = headers.index("url")
    except ValueError:
        print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'URL' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    # status_columns = ['id', 'Status', 'Created', 'Updated', 'Comment']

    # for col in status_columns:
    #     if col.lower() not in headers:
    #         headers.append(col)

    #     if len(headers) > admin_main.col_count:
    #         admin_main.add_cols(len(headers) - admin_main.col_count)

    #     admin_main.update("1:1", [headers])

    for i, row in enumerate(rows):
        if name_col < len(row) and url_col < len(row):
            client_name = row[name_col].strip()
            client_url = row[url_col].strip()
            if client_name and client_url:
                asyncio.run(create_client(i, client_name, client_url, admin_main, headers))
                
        

if __name__ == "__main__":
    main()
